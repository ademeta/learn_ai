{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19aa5d3a-8516-42b5-9c7e-8ba95407e750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "\n",
    "import torch\n",
    "import os,sys\n",
    "import gc\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf585344-9d7a-4526-86ef-5c9175483bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944150c7-37c0-4b84-a1cc-105dcc1ab71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74097536-31a3-461b-add5-097e50bcd87d",
   "metadata": {},
   "source": [
    "### Llama base model \n",
    "\n",
    "Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c12c2f-96d1-4054-bb3e-8dff45d92b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521ad0b6-e267-4666-997a-360318675e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 15836, 17005, 374, 8056]\n"
     ]
    }
   ],
   "source": [
    "text= \"AI Engineering is amazing\"\n",
    "tokens= tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c722d15-e779-4ed6-97ab-cd117d3c2fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>AI Engineering is amazing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f270ee-6139-4be2-8c95-00dad3bb718d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>AI Engineering is amazing']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a847b-07c5-446f-b508-14a21a36e0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8390b03f-28bb-47dd-baff-b030356751cb",
   "metadata": {},
   "source": [
    "### Llama instruct model\n",
    "\n",
    "Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cc1585-4ca8-4aa8-b48d-ba01792b5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama instruct model\n",
    "# instruct models - base models fine-tuned to act in form of question-answering\n",
    "# like in the openai api, we can also apply chat template with the instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "673f85d0-1e83-49dd-b41b-f327a986bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38788d82-e23e-4759-8a65-b899bd0a97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke about monthly salary earners\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2d1c0f-1317-4d14-b915-26a77b38b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You're a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a joke about monthly salary earners<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f585ba2-dffb-4a29-b490-5ddf04111ce9",
   "metadata": {},
   "source": [
    "##### Models and their templates.\n",
    "\n",
    "When we apply chat templates on messages, we are letting it assume the structure of training data used when fine-tuning base models to instruct models. \n",
    "\n",
    "Essentially, base models were pre-trained to predict next words. Instruct models are a variant of the base model fine-tuned for end users to have the ability to interact with LLMs and the models producing meaningful responses like ChatGPT style.\n",
    "\n",
    "The Llama chat template applied on the message above has added extra start of sentence, end of sentence tokens, tags to separate users and assistant in a way the models understand. \n",
    "\n",
    "We will see what this looks like with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097a9f8-2d89-4fc8-bc02-e05cb3c189d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "033818df-f5b1-412c-848b-6c9f05a1e3c8",
   "metadata": {},
   "source": [
    "#### Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752f5667-326a-4b8e-8663-d9a9b42590f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi4 = \"microsoft/Phi-4-mini-instruct\"\n",
    "deepseek = \"deepseek-ai/DeepSeek-V3.1\"\n",
    "qwen_coder = \"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2e800b-d4b7-41c8-9cb7-373fee07dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This model config has set a `rope_parameters['original_max_position_embeddings']` field, to be used together with `max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, as it is compatible with most model architectures.\n",
      "`rope_parameters`'s factor field must be a float >= 1, got 40\n",
      "`rope_parameters`'s beta_fast field must be a float, got 32\n",
      "`rope_parameters`'s beta_slow field must be a float, got 1\n"
     ]
    }
   ],
   "source": [
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi4)\n",
    "ds_tokenizer = AutoTokenizer.from_pretrained(deepseek)\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf36a29-e670-43cc-8c0c-bc5d1739923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi4: \n",
      " <|system|>You're a helpful assistant<|end|><|user|>Tell a joke about monthly salary earners<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "prompt = phi_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"phi4: \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059c72a7-4415-491c-82f9-3b63d52bf0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek: \n",
      " <｜begin▁of▁sentence｜>You're a helpful assistant<｜User｜>Tell a joke about monthly salary earners<｜Assistant｜></think>\n"
     ]
    }
   ],
   "source": [
    "prompt = ds_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"deepseek: \\n\",prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff14aa46-f88b-4d51-8cad-ead34dbce0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen_coder: \n",
      " <|im_start|>system\n",
      "You're a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell a joke about monthly salary earners<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = qwen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"qwen_coder: \\n\",prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decabdab-a4a1-4e37-b3db-242d3a2882f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cdbf486-e346-4034-8852-39c85c17c521",
   "metadata": {},
   "source": [
    "### Generating with HF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e731b2db-3163-4149-b54a-487d010a4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7133f0b-1cff-4581-a8e1-ba2adad33cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize the model to load and use less memory\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d77411-5e43-47e9-81e5-fc8439a76ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e074e152-e5eb-4d42-ac81-e09a2297632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b24bbc9b-1f97-4ee4-8557-1446ebb14bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,   2351,\n",
       "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  41551,\n",
       "            264,  22380,    922,  15438,  16498,  97063, 128009]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32546c6-32d3-4204-bdab-e53078894d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbece3e60dcd408e8492f56a7c4691d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "146534c7-488d-4879-9341-3974cbb92027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb157093-6ffc-401d-af73-9654cd9ad33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,   2351,\n",
       "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  41551,\n",
       "            264,  22380,    922,  15438,  16498,  97063, 128009]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bca62ce1-9d19-4878-a0cf-27f6f6065f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,   2351,\n",
       "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  41551,\n",
       "            264,  22380,    922,  15438,  16498,  97063, 128009, 128006,  78191,\n",
       "         128007,    271,  10445,   1550,    279,  15438,  16498,   2487,   1215,\n",
       "           4546,    264,  36865,    311,    279,   6201,   1980,  18433,    814,\n",
       "           4934,    311,   1935,    872,  16498,    311,    279,   1828,   2237,\n",
       "             13, 128009]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs.input_ids, max_new_tokens=120)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399b499a-3500-4ed6-a9cb-d7b555d95a02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m.decode(outputs[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e745998-337c-4527-a68f-bd69842d7875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f62e8d-5e99-4a0f-9f0c-23d6b083e105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dd91d18-93ca-4a48-9f8a-aa39edae426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up disk\n",
    "\n",
    "# del model, inputs, tokenizer, outputs\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6ffe865-90eb-436c-a630-b82d419a42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, inputs, tokenizer, outputs\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faea66f8-4249-4581-b89f-07b8a3ffa03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,   2351,\n",
       "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  41551,\n",
       "            264,  22380,    922,  15438,  16498,  97063, 128009, 128006,  78191,\n",
       "         128007,    271,  10445,   1550,    279,  15438,  16498,   2487,   1215,\n",
       "           4546,    264,  36865,    311,    279,   6201,   1980,  18433,    814,\n",
       "           4934,    311,   1935,    872,  16498,    311,    279,   1828,   2237,\n",
       "             13, 128009]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f810f91a-880f-4af9-83cb-f2f3d6ef479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, messages, quant=True, max_new_tokens=80):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "    attention_mask = torch.ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b73785-5c4d-4f01-b490-8d938685a4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
